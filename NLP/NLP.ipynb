{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP12YHatckzZEaMxhRk0urW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deethereal/NN/blob/main/NLP/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uog6C8EtUARj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e5e385-1ee6-4840-9e37-8ca65957ab54"
      },
      "source": [
        "import numpy as np, pandas as pd\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0etihaAHPoS",
        "outputId": "ad5e6c85-859d-4aab-d356-7b0ee84f2f38"
      },
      "source": [
        "\r\n",
        "onehots={}\r\n",
        "onehots['cat']=np.array([1,0,0,0])\r\n",
        "onehots['the']=np.array([0,1,0,0])\r\n",
        "onehots['dog']=np.array([0,0,1,0])\r\n",
        "onehots['sat']=np.array([0,0,0,1])\r\n",
        "\r\n",
        "sentence=['the','cat','sat','sat','dog']\r\n",
        "\r\n",
        "x=onehots[sentence[0]]\r\n",
        "for i in range(1,len(sentence)):\r\n",
        "  x+=onehots[sentence[i]]\r\n",
        "print(str(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a6rzGwlHcM1",
        "outputId": "e90749c7-1383-4861-d32d-944b7baa7ad2"
      },
      "source": [
        "#lines=csv.DictReader(\"/content/gdrive/My Drive/IMDB.csv/\",delimiter=',')\r\n",
        "%cd \"/content/drive/My Drive/\"\r\n",
        "lines=pd.read_csv('IMDB.csv', sep=',')\r\n",
        "for line in lines:\r\n",
        "  raw_reviews, raw_labels= lines['review'], lines['sentiment']\r\n",
        "tokens=list(map(lambda x:set(x.split(\" \")),raw_reviews))\r\n",
        "vocab=set()\r\n",
        "for sent in tokens:\r\n",
        "  for word in sent:\r\n",
        "    if len(word)>0:\r\n",
        "      vocab.add(word)\r\n",
        "vocab=list(vocab)\r\n",
        "word2index={}\r\n",
        "for i, word in enumerate(vocab):\r\n",
        "  word2index[word]=i\r\n",
        "input_dataset=list()\r\n",
        "for sent in tokens:\r\n",
        "  sent_indices = list()\r\n",
        "  for word in sent:\r\n",
        "    try:\r\n",
        "      sent_indices.append(word2index[word])\r\n",
        "    except:\r\n",
        "      \"\"\r\n",
        "  input_dataset.append(list(set(sent_indices)))\r\n",
        "target_dataset=list()\r\n",
        "for label in raw_labels:\r\n",
        "  if label == 'positive':\r\n",
        "    target_dataset.append(1)\r\n",
        "  else:\r\n",
        "    target_dataset.append(0)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXVj20xjlRxc",
        "outputId": "83696425-5feb-4614-c30b-288d00223ffd"
      },
      "source": [
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "439838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "won3RNST9B2O",
        "outputId": "9298ac0b-e99b-4c9c-cd96-6cfc45b5c21c"
      },
      "source": [
        "print(len(input_dataset[2]),len(raw_reviews[2].split()))\r\n",
        "print(raw_reviews[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "121 166\n",
            "I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsk0f6gbnn15",
        "outputId": "58f2b0ed-c471-4866-b0b3-673d90f6eaa1"
      },
      "source": [
        "S=len(input_dataset)\r\n",
        "print(S)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-brZHUzBlix5",
        "outputId": "48363715-039c-45c4-c1a7-c8623c4cd9fb"
      },
      "source": [
        "np.random.seed(1)\r\n",
        "def sigmoid(x):\r\n",
        "  return 1/(1+np.exp(-x))\r\n",
        "learning_rate, niter, hidden_size= 0.01, 35, 150\r\n",
        "W01=0.2*np.random.random((len(vocab),hidden_size))-0.1\r\n",
        "W12=0.2*np.random.random((hidden_size,1))-0.1\r\n",
        "correct, total = 0,0\r\n",
        "for iter in range(niter):\r\n",
        "  for i in range(len(input_dataset)-2000):\r\n",
        "    x,y=(input_dataset[i],target_dataset[i])\r\n",
        "    l1=sigmoid(np.sum(W01[x],axis=0))\r\n",
        "    l2=sigmoid(np.dot(l1,W12))\r\n",
        "  \r\n",
        "\r\n",
        "    l2_delta=l2-y\r\n",
        "    l1_delta=l2_delta.dot(W12.T)\r\n",
        "\r\n",
        "    W01[x]-=l1_delta*learning_rate\r\n",
        "    W12-=np.outer(l1,l2_delta)*learning_rate\r\n",
        "\r\n",
        "    if (np.abs(l2_delta) < 0.5):\r\n",
        "      correct+=1\r\n",
        "    total+=1\r\n",
        "    \"\"\"if (i% 10 == 9):\r\n",
        "      progress=str(i/float(len(input_dataset)-26000))\r\n",
        "      print(\"Iter: {} Progress: {}.{} TrainingAccuracy: {}\".format(iter,progress[2:4],progress[4:6],(correct/float(total))))\"\"\"\r\n",
        "correct, total = 0,0\r\n",
        "for i in range(len(input_dataset)-2000,len(input_dataset)):\r\n",
        "  x,y=(input_dataset[i],target_dataset[i])\r\n",
        "  l1=sigmoid(np.sum(W01[x],axis=0))\r\n",
        "  l2=sigmoid(np.dot(l1,W12))\r\n",
        "    \r\n",
        "  if np.abs(l2-y)<0.1:\r\n",
        "    correct+=1\r\n",
        "  total+=1\r\n",
        "print('Test Accuracy: {}%'.format(100*correct/float(total)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 81.75%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-igFvvjBGW6k"
      },
      "source": [
        "def nn(x):\r\n",
        "  l1=sigmoid(np.sum(W01[x],axis=0))\r\n",
        "  l2=sigmoid(np.dot(l1,W12))\r\n",
        "  print(l2)\r\n",
        "  print(\"The nature of review is\", end=\" \")\r\n",
        "  print(\"positive\") if l2>0.5 else print(\"negative\")\r\n",
        "def proccecing(x):\r\n",
        "  x=list(set(x.split(\" \")))\r\n",
        "  sent_indices = list()\r\n",
        "  for word in x:\r\n",
        "    try:\r\n",
        "      sent_indices.append(word2index[word])\r\n",
        "    except:\r\n",
        "      \"\"\r\n",
        "  return list(set(sent_indices))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkLi-alTIQu_",
        "outputId": "0cc69479-f124-4799-c892-af9c7ba33a8b"
      },
      "source": [
        "mr1=\"The film is very boring, I kept waiting for it to start !\"\r\n",
        "nn(proccecing(mr1))\r\n",
        "mr2=\"I really liked this film, almost everything is great in it: the acting, the plot and the special effects !\"\r\n",
        "nn(proccecing(mr2))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00367403]\n",
            "The nature of review is negative\n",
            "[0.99998366]\n",
            "The nature of review is positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OYRJ7cISbpE"
      },
      "source": [
        "***Эта архитектура ищет прямую корреляцию между входом и выходом***  \r\n",
        "Поэтому получается так:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isSmGlNGSbDP",
        "outputId": "7b23670f-03ba-4dce-e4e7-be663e868c5b"
      },
      "source": [
        "test1='good film not bad'\r\n",
        "test2='bad film not good'\r\n",
        "nn(proccecing(test1))\r\n",
        "nn(proccecing(test2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.11175849e-05]\n",
            "The nature of review is negative\n",
            "[3.11175849e-05]\n",
            "The nature of review is negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzFJnj5LzuSc"
      },
      "source": [
        "from collections import Counter\r\n",
        "def similiar(target='beautiful'):\r\n",
        "  target_index=word2index[target]\r\n",
        "  scores=Counter()\r\n",
        "  for word, index in word2index.items():\r\n",
        "    raw_diff=W01[index]-W01[target_index]\r\n",
        "    sqr=raw_diff*raw_diff\r\n",
        "    scores[word]=-np.sqrt(sum(sqr))\r\n",
        "  return scores.most_common(10)\r\n",
        "def sigmoid(x):\r\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ebgj0jNv1sYp",
        "outputId": "c19add62-3446-419f-b6e9-aca99ad03016"
      },
      "source": [
        "print(similiar('Marvel'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Marvel', -0.0), ('ledger:', -0.37299359811339494), ('Marine(Naval', -0.37590073475230473), ('Peirce', -0.37950251177209005), ('\"Heaven\\'s', -0.37995265294910785)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o2xkXU9KmcW",
        "outputId": "5cd2d912-b748-4674-f356-aa93f1b305ef"
      },
      "source": [
        "np.random.seed(1)\r\n",
        "%cd \"/content/drive/My Drive/\"\r\n",
        "lines=pd.read_csv('IMDB.csv', sep=',')\r\n",
        "for line in lines:\r\n",
        "  raw_reviews, raw_labels= lines['review'], lines['sentiment']\r\n",
        "tokens=list(map(lambda x:set(x.split(\" \")),raw_reviews))\r\n",
        "wordcnt= Counter() \r\n",
        "for sent in tokens:\r\n",
        "  for word in sent:\r\n",
        "    wordcnt[word]-=1\r\n",
        "vocab=list(set(map(lambda x:x[0],wordcnt.most_common())))\r\n",
        "\r\n",
        "\r\n",
        "word2index={}\r\n",
        "for i,word in enumerate(vocab):\r\n",
        "  word2index[word]=i\r\n",
        "\r\n",
        "concatenated = list()\r\n",
        "input_dataset = list()\r\n",
        "for sent in tokens:\r\n",
        "  sent_indices=list()\r\n",
        "  for word in sent:\r\n",
        "    try:\r\n",
        "      sent_indices.append(word2index[word])\r\n",
        "      concatenated.append(word2index[word])\r\n",
        "    except:\r\n",
        "      \"\"\r\n",
        "  input_dataset.append(sent_indices)\r\n",
        "concatenated=np.array(concatenated)\r\n",
        "\r\n",
        "np.random.shuffle(input_dataset)\r\n",
        "print(\"Data is ready!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n",
            "Data is ready!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LrN4DjlbScI",
        "outputId": "8455eee8-3298-4100-c244-74460837043e"
      },
      "source": [
        "learning_rate, hidden_size, niter = 0.05,50,5\r\n",
        "window,negative = 2,5\r\n",
        "\r\n",
        "W01=0.2*(np.random.random((len(vocab),hidden_size))-0.5)\r\n",
        "W12=0.2*(np.random.random((len(vocab),hidden_size,)) -0.5)\r\n",
        "l2_target=np.zeros(negative+1)\r\n",
        "l2_target[0]=1\r\n",
        "progress_l=-1\r\n",
        "\r\n",
        "\r\n",
        "for rev_i, review in enumerate(input_dataset*niter):\r\n",
        "  for target_i in range(len(review)):\r\n",
        "    target_sample=[review[target_i]]+list(concatenated[(np.random.rand(negative)*len(concatenated)).astype('int').tolist()])  #выбираем случайное подмонжество\r\n",
        "\r\n",
        "\r\n",
        "    left_context=review[max(0,target_i-window):target_i]\r\n",
        "    right_context=review[target_i+1:min(len(review),target_i+window)]\r\n",
        "\r\n",
        "\r\n",
        "    l1=np.mean(W01[left_context+right_context], axis=0)\r\n",
        "    l2=sigmoid(l1.dot(W12[target_sample].T))\r\n",
        "\r\n",
        "    l2_delta=l2-l2_target\r\n",
        "    l1_delta=l2_delta.dot(W12[target_sample])\r\n",
        "\r\n",
        "    W01[left_context+right_context]-=l1_delta*learning_rate\r\n",
        "    W12[target_sample]-=np.outer(l2_delta,l1)*learning_rate\r\n",
        "  current_p=round(rev_i/float(len(input_dataset)),1)  \r\n",
        "  if progress_l<current_p:\r\n",
        "    print('Progress: {} '.format(current_p))\r\n",
        "    progress_l=current_p \r\n",
        "print(similiar('terrible'))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: 0.0 \n",
            "Progress: 0.1 \n",
            "Progress: 0.2 \n",
            "Progress: 0.3 \n",
            "Progress: 0.4 \n",
            "Progress: 0.5 \n",
            "Progress: 0.6 \n",
            "Progress: 0.7 \n",
            "Progress: 0.8 \n",
            "Progress: 0.9 \n",
            "Progress: 1.0 \n",
            "Progress: 1.1 \n",
            "Progress: 1.2 \n",
            "Progress: 1.3 \n",
            "Progress: 1.4 \n",
            "Progress: 1.5 \n",
            "Progress: 1.6 \n",
            "Progress: 1.7 \n",
            "Progress: 1.8 \n",
            "Progress: 1.9 \n",
            "Progress: 2.0 \n",
            "Progress: 2.1 \n",
            "Progress: 2.2 \n",
            "Progress: 2.3 \n",
            "Progress: 2.4 \n",
            "Progress: 2.5 \n",
            "Progress: 2.6 \n",
            "Progress: 2.7 \n",
            "Progress: 2.8 \n",
            "Progress: 2.9 \n",
            "Progress: 3.0 \n",
            "Progress: 3.1 \n",
            "Progress: 3.2 \n",
            "Progress: 3.3 \n",
            "Progress: 3.4 \n",
            "Progress: 3.5 \n",
            "Progress: 3.6 \n",
            "Progress: 3.7 \n",
            "Progress: 3.8 \n",
            "Progress: 3.9 \n",
            "Progress: 4.0 \n",
            "Progress: 4.1 \n",
            "Progress: 4.2 \n",
            "Progress: 4.3 \n",
            "Progress: 4.4 \n",
            "Progress: 4.5 \n",
            "Progress: 4.6 \n",
            "Progress: 4.7 \n",
            "Progress: 4.8 \n",
            "Progress: 4.9 \n",
            "Progress: 5.0 \n",
            "[('terrible', -0.0), ('rival', -4.199290883535479), ('sister.', -4.508783023488256), ('Except', -4.545213670036982), ('assuming', -4.546449340962891), ('guys.', -4.624140835325655), ('So', -4.764357390823417), ('average.', -4.842264849255279), ('trash,', -4.8577642654631354), ('cartoon.', -4.8707227763887095)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiKZOYC4hb7b",
        "outputId": "56df9eb1-a406-4caa-b804-a089c6f80488"
      },
      "source": [
        "print(similiar('good'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('good', -0.0), ('video,', -6.514949966016076), ('doubt,', -6.759650566502447), ('intention', -6.8007262864054345), ('contribution', -6.811713649884149), ('/>John', -6.859748670258398), ('mess,', -6.95821079913052), ('documentary', -7.022108610069692), ('river', -7.080200197335381), ('passing', -7.083841749468614)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRJjKQRhyGlx",
        "outputId": "b657c946-b4b8-49e3-b4a9-3add5ca2da91"
      },
      "source": [
        "def analogy(pos=[ 'terrible','good' ], negative = ['bad'] ):\r\n",
        "  norms=np.sum(W01*W01,axis=1)\r\n",
        "  norms.resize(norms.shape[0],1)\r\n",
        "\r\n",
        "  normed_W=W01*norms\r\n",
        "  query_vect = np.zeros(len(W01[0]))\r\n",
        "  for word in pos:\r\n",
        "    query_vect+=normed_W[word2index[word]]\r\n",
        "  for word in negative:\r\n",
        "    query_vect-=normed_W[word2index[word]]\r\n",
        "  scores = Counter()\r\n",
        "  for word, index in word2index.items():\r\n",
        "    raw_diff= W01[index]-query_vect\r\n",
        "    sqr=raw_diff*raw_diff\r\n",
        "    scores[word]=np.sqrt(sum(sqr))\r\n",
        "  return scores.most_common(10)[1:]\r\n",
        "print(analogy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('sense', 1097.1943195290353), ('masterpiece', 1097.1491971455425), ('ordinary', 1097.0758389074565), ('tension.', 1097.0502683700718), ('relief', 1096.9729424463587), ('ultimate', 1096.9243475543674), ('genius.', 1096.8958663933558), ('Picture', 1096.8765726437382), ('boys.', 1096.8714072843113)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwjBMksUrgKO"
      },
      "source": [
        "import numpy as np\r\n",
        "def softmax(x_):\r\n",
        "  x=np.atleast_2d(x_)\r\n",
        "  temp=np.exp(x)\r\n",
        "  return temp/np.sum(temp,axis=1,keepdims=True)\r\n",
        "word_vects={}\r\n",
        "word_vects['yankes']=np.array([[0.,0.,0.]])\r\n",
        "word_vects['bears']=np.array([[0.,0.,0.]])\r\n",
        "word_vects['braves']=np.array([[0.,0.,0.]])\r\n",
        "word_vects['red']=np.array([[0.,0.,0.]])\r\n",
        "word_vects['sox']=np.array([[0.,0.,0.]])\r\n",
        "word_vects['lose']=np.array([[0.,0.,0.]])\r\n",
        "word_vects['defeat']=np.array([[0.,0.,0.]])\r\n",
        "word_vects['beat']=np.array([[0.,0.,0.]])\r\n",
        "word_vects['tie']=np.array([[0.,0.,0.]])\r\n",
        "\r\n",
        "sent2output=np.random.rand(3,len(word_vects))\r\n",
        "\r\n",
        "identity=np.eye(3)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0OR04JutVqi",
        "outputId": "b7a25a5a-b5b6-4c67-baf1-bd8982a5c6ae"
      },
      "source": [
        "l0=word_vects['red']\r\n",
        "l1=l0.dot(identity)+word_vects['sox']\r\n",
        "l2=l1.dot(identity)+word_vects['defeat']\r\n",
        "\r\n",
        "pred=softmax(l2.dot(sent2output))\r\n",
        "print(pred)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
            "  0.11111111 0.11111111 0.11111111]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXRbhFfguHMo"
      },
      "source": [
        "yankes=bp.array([1,0,0,0,0,0,0,0,0])\r\n",
        "pred_delta=pred-y\r\n",
        "l2_delta=pred_delta.dot(sent2output.T)\r\n",
        "defeat_delta=l2_delta\r\n",
        "l1_delta=l2_delta.dot(identity.T)\r\n",
        "sox_delta=l1_delta\r\n",
        "l0_delta=l1_delta.dot(identity.T)\r\n",
        "lr=0.01\r\n",
        "word_vects['red']-=l0_delta*lr \r\n",
        "word_vects['sox']-=sox_delta*lr \r\n",
        "word_vects['defeat']-=defeat_delta*lr \r\n",
        "identity-=np.outer(l0,l1_delta)*lr \r\n",
        "identity-=np.outer(l1,l2_delta)*lr \r\n",
        "sent2output-=np.outer(l2,pred_delta) *lr \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMkVVGOHwenm"
      },
      "source": [
        "!wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-1.tar.gz\r\n",
        "!tar -xvf tasks_1-20_v1-1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAQg-5idw_It",
        "outputId": "9fee4e15-f6b3-47ed-9709-48fe01aa064e"
      },
      "source": [
        "import sys,random,math\r\n",
        "from collections import Counter\r\n",
        "import numpy as np\r\n",
        "f=open('tasksv11/en/qa1_single-supporting-fact_train.txt','r')\r\n",
        "raw=f.readlines()\r\n",
        "f.close()\r\n",
        "tokens=list()\r\n",
        "for line in raw[0:1000]:\r\n",
        "  tokens.append(line.lower().replace('\\n','').replace('?','').replace('\\t','').split(' ')[1:])\r\n",
        "for t in tokens:\r\n",
        "  while t[3][-1] in ['1','2','3','4','5','6','7','8','9']:\r\n",
        "    t[3]=t[3][:-1]\r\n",
        "for i in range(0,33,3):\r\n",
        "  print(tokens[i],tokens[i+1],tokens[i+2])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['mary', 'moved', 'to', 'the', 'bathroom.'] ['john', 'went', 'to', 'the', 'hallway.'] ['where', 'is', 'mary', 'bathroom']\n",
            "['daniel', 'went', 'back', 'to', 'the', 'hallway.'] ['sandra', 'moved', 'to', 'the', 'garden.'] ['where', 'is', 'daniel', 'hallway']\n",
            "['john', 'moved', 'to', 'the', 'office.'] ['sandra', 'journeyed', 'to', 'the', 'bathroom.'] ['where', 'is', 'daniel', 'hallway']\n",
            "['mary', 'moved', 'to', 'the', 'hallway.'] ['daniel', 'travelled', 'to', 'the', 'office.'] ['where', 'is', 'daniel', 'office']\n",
            "['john', 'went', 'back', 'to', 'the', 'garden.'] ['john', 'moved', 'to', 'the', 'bedroom.'] ['where', 'is', 'sandra', 'bathroom']\n",
            "['sandra', 'travelled', 'to', 'the', 'office.'] ['sandra', 'went', 'to', 'the', 'bathroom.'] ['where', 'is', 'sandra', 'bathroom']\n",
            "['mary', 'went', 'to', 'the', 'bedroom.'] ['daniel', 'moved', 'to', 'the', 'hallway.'] ['where', 'is', 'sandra', 'bathroom']\n",
            "['john', 'went', 'to', 'the', 'garden.'] ['john', 'travelled', 'to', 'the', 'office.'] ['where', 'is', 'sandra', 'bathroom']\n",
            "['daniel', 'journeyed', 'to', 'the', 'bedroom.'] ['daniel', 'travelled', 'to', 'the', 'hallway.'] ['where', 'is', 'john', 'office']\n",
            "['john', 'went', 'to', 'the', 'bedroom.'] ['john', 'travelled', 'to', 'the', 'office.'] ['where', 'is', 'daniel', 'hallway']\n",
            "['mary', 'went', 'to', 'the', 'bedroom.'] ['john', 'journeyed', 'to', 'the', 'bathroom.'] ['where', 'is', 'john', 'bathroom']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4ldP3iz1aFI"
      },
      "source": [
        "vocab=set()\r\n",
        "for sent in tokens:\r\n",
        "  for word in sent:\r\n",
        "    vocab.add(word)\r\n",
        "vocab=list(vocab)\r\n",
        "word2index={}\r\n",
        "for i,word in enumerate(vocab):\r\n",
        "  word2index[word]=i\r\n",
        "def word2indices(sentence):\r\n",
        "  idx=list()\r\n",
        "  for word in sentence:\r\n",
        "    idx.append(word2index[word])\r\n",
        "  return idx\r\n",
        "def softmax(x):\r\n",
        "  e_x=np.exp(x-np.max(x))\r\n",
        "  return e_x/e_x.sum(axis=0)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRu8UNq72PYQ"
      },
      "source": [
        "def predict(sent):\r\n",
        "  layers=list()\r\n",
        "  layer={}\r\n",
        "  layer['hidden']=start\r\n",
        "  layers.append(layer)\r\n",
        "  loss=0\r\n",
        "  preds=list()\r\n",
        "  for target_i in range(len(sent)):\r\n",
        "    layer={}\r\n",
        "    layer['pred']=softmax(layers[-1]['hidden'].dot(decoder))\r\n",
        "    loss+=-np.log(layer['pred'][sent[target_i]])\r\n",
        "    layer['hidden']=layers[-1]['hidden'].dot(reccurent)+embed[sent][target_i]\r\n",
        "    layers.append(layer)\r\n",
        "  return layers,loss\r\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SauLCgUe3_rL",
        "outputId": "8d565f29-5f8e-40b6-9864-f40111a1801c"
      },
      "source": [
        "np.random.seed(1)\r\n",
        "embed_size=10\r\n",
        "embed=(np.random.rand(len(vocab),embed_size)-0.5)*0.1\r\n",
        "reccurent=np.eye(embed_size)\r\n",
        "start=np.zeros(embed_size)\r\n",
        "decoder=(np.random.rand(embed_size,len(vocab))-0.5)*0.1\r\n",
        "one_hot = np.eye(len(vocab))\r\n",
        "learning_rate=0.01\r\n",
        "for iter in range(38000):\r\n",
        "  sent=word2indices(tokens[iter%len(tokens)][1:])\r\n",
        "  layers,loss=predict(sent)\r\n",
        "  for layer_idx in reversed(range(len(layers))):\r\n",
        "    layer=layers[layer_idx]\r\n",
        "    target = sent[layer_idx-1]\r\n",
        "    if (layer_idx>0):\r\n",
        "      layer['output_delta']=layer['pred']-one_hot[target]\r\n",
        "      new_hidden_delta=layer['output_delta'].dot(decoder.T)\r\n",
        "      if (layer_idx==len(layers)-1):\r\n",
        "        layer['hidden_delta']=new_hidden_delta\r\n",
        "      else:\r\n",
        "        layer['hidden_delta']=new_hidden_delta+layers[layer_idx+1]['hidden_delta'].dot(reccurent.T)\r\n",
        "    else:\r\n",
        "      layer['hidden_delta']=layers[layer_idx+1]['hidden_delta'].dot(reccurent.T)    \r\n",
        "  start-=layers[0]['hidden_delta']*learning_rate/float(len(sent))\r\n",
        "  for layer_idx, layer in enumerate(layers[1:]):\r\n",
        "    decoder-=np.outer(layers[layer_idx]['hidden'],layer['output_delta'])*learning_rate/float(len(sent))\r\n",
        "    embed_idx=sent[layer_idx] \r\n",
        "    embed[embed_idx]-=layers[layer_idx]['hidden_delta']*learning_rate/float(len(sent))\r\n",
        "    reccurent-=np.outer(layers[layer_idx]['hidden_delta'], layer['hidden_delta'])*learning_rate/float(len(sent))\r\n",
        "  if iter%1000 ==0:\r\n",
        "    print(\"Перплексия: \" +str(np.exp(loss/len(sent))))   \r\n",
        "\r\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Перплексия: 30.963606957387032\n",
            "Перплексия: 27.71866579607081\n",
            "Перплексия: 24.595649659970682\n",
            "Перплексия: 21.724290570494933\n",
            "Перплексия: 18.246991785631984\n",
            "Перплексия: 13.948758743495924\n",
            "Перплексия: 9.138576006070066\n",
            "Перплексия: 8.039250038607824\n",
            "Перплексия: 7.911053190462284\n",
            "Перплексия: 8.572291615523767\n",
            "Перплексия: 10.471806460466208\n",
            "Перплексия: 10.998395084030497\n",
            "Перплексия: 9.489275387261065\n",
            "Перплексия: 7.79456714675163\n",
            "Перплексия: 6.156219912462877\n",
            "Перплексия: 5.752824356611411\n",
            "Перплексия: 6.953222861265637\n",
            "Перплексия: 7.378860344340722\n",
            "Перплексия: 6.258279108279967\n",
            "Перплексия: 5.211686625632706\n",
            "Перплексия: 5.3091401158635305\n",
            "Перплексия: 5.825962511525318\n",
            "Перплексия: 6.4187292737531685\n",
            "Перплексия: 6.672027494036467\n",
            "Перплексия: 6.357797371479096\n",
            "Перплексия: 5.827879574224959\n",
            "Перплексия: 5.364145952938768\n",
            "Перплексия: 5.247328339466191\n",
            "Перплексия: 5.363271149647524\n",
            "Перплексия: 5.753115906033597\n",
            "Перплексия: 6.081939027861756\n",
            "Перплексия: 5.361506250765158\n",
            "Перплексия: 4.514980863514879\n",
            "Перплексия: 4.237000694350489\n",
            "Перплексия: 4.016248571375177\n",
            "Перплексия: 3.71599819816935\n",
            "Перплексия: 3.2694628291838694\n",
            "Перплексия: 3.1333397791071707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v3_diTMCa8U",
        "outputId": "7ef7ad70-6e09-4a8b-848d-836903071f6a"
      },
      "source": [
        "sent_index=4\r\n",
        "l,_=predict(word2indices(tokens[sent_index]))\r\n",
        "print(tokens[sent_index])\r\n",
        "for i,each_layer in enumerate(l[1:-1]):\r\n",
        "  input=tokens[sent_index][i]\r\n",
        "  true=tokens[sent_index][i+1]\r\n",
        "  pred=vocab[each_layer['pred'].argmax()]\r\n",
        "  print('Предыдущее слово: '+input+(\" \"*(12-len(input)))+\"Ожидание: \"+true+(\" \"*(15-len(true))) + \"Предсказание: \"+pred )"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sandra', 'moved', 'to', 'the', 'garden.']\n",
            "Предыдущее слово: sandra      Ожидание: moved          Предсказание: is\n",
            "Предыдущее слово: moved       Ожидание: to             Предсказание: to\n",
            "Предыдущее слово: to          Ожидание: the            Предсказание: to\n",
            "Предыдущее слово: the         Ожидание: garden.        Предсказание: the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhgFybFyEL8f"
      },
      "source": [
        ""
      ],
      "execution_count": 54,
      "outputs": []
    }
  ]
}